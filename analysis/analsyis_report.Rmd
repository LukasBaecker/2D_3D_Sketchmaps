---
title: "Comparing 2D and 3D Sketch Maps in Virtual Reality"
author: "Lukas Bäcker"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load libraries, message = FALSE, echo=FALSE, eval = FALSE}
#Run this if you have all the packages, otherwise run the chunk below:

library(tidyverse)
library(here)
library(plotly)
library(sf)
library(knitr)
library(data.table)
library(lme4)
library(lmerTest)
library(flexplot)
library(MuMIn)
library(effects)
library(ggplot2)
library(sjPlot)
library(sjstats)
library(reshape2)

```

```{r Install and load missing packages, include = FALSE}
#Run this chunk to install and load packages that are missing. 

#installing flexplot from github
devtools::install_github("dustinfife/flexplot")

# Necessary packages for script.
packages = c("tidyverse", "here",
             "plotly", "sf", "knitr", "data.table", "lme4", "lmerTest", "MuMIn", "effects", "ggplot2","sjPlot", "reshape2", "sjstats")

# Install and then load them.
package.check <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  }
)
```

```{r load typeform}
# Info for the csv:
# ID is the ID of the participant
# Age is the age of the participant
# Gender is a number from 1 to 4 where 1 means male, 2 means femal, 3 means diverse and 4 means undefined
# Q1 to Q10 are the answers for the IPT
# Order defines the order of drawing tasks. 1 means that the person first drew with pen and paper and 2 means that the person first drew in gravity sketch 
# PnP_1 to PnP_6 is the NASA-TLX for pen and paper drawing
# VR_1 to VR_6 is the NASA-TLX for drawing with gravity sketch

questionair <- read.csv(here("data/questionair.csv"))
participant_count <- nrow(questionair) # count the number of participants
```

```{r basic statistics}

# age statistics
age_mean <- mean(questionair$Age)
age_min <- min(questionair$Age)
age_max <- max(questionair$Age)

boxplot(questionair$Age) #visualize the age distribution

# table showing the gender contribution
gender_table <- table(questionair$Gender)

```


## Indoor Perspective Test

In this section the Indoor Perspective Test (IPT) will be analysed. 10 questions where asked and the overall score for one person is the number of correct answers. A maximum score of 10 and a minimum of 0 can be expected.

```{r IPT}
solution <- c(3,1,4,3,4,3,2,2,2,1) # the correct answers for the IPT
ipt_results <- vector(length=participant_count) # create a vector for saving the IPT results where the index is the ID of the participant

for (index in 1:participant_count) {
  counter <- 0 # counting right answers
  for(q in 1:10){
    if(questionair[index,paste("Q",q, sep = "")]==solution[q]){
      counter<- counter+1
    }
  }
  ipt_results[index] <- counter #safe results in an vector where the index is the ID of the participant
}

ipt_mean <- mean(ipt_results)
boxplot(ipt_results)
```


## NASA-TLX Analysis

```{r NASA-TLX}
#TODO: analyse the NASA-TLX with an overall value
#TODO: distinguish between the mental and the physical demand here
#Q1: Mental demand from not demanding to highly demanding
#Q2: Physiscal Demand from not demanding to highly demanding
#Q3: Time Requ from not time consuming to highly time consuming
#Q4: Performance Satifcation from not at all satisfied to totally satisfied
#Q5: How much effort and how hard had you to work from not hard at all to very hard
#Q6: Frustration from not frustrated at all to highly frustrated

TLX.perQuestion <- matrix(nrow = 6, ncol = 3, byrow = FALSE)
colnames(TLX.perQuestion) <-  c("Question","Pen and Paper", "VR")
TLX.perQuestion_dataframe <- as.data.frame(TLX.perQuestion)

TLX.perQuestion_std <- matrix(nrow = 6, ncol = 3, byrow = FALSE)
colnames(TLX.perQuestion_std) <-  c("Question","Pen and Paper", "VR")
TLX.perQuestion_std_dataframe <- as.data.frame(TLX.perQuestion_std)

TLX.perQuestion_std_list <- vector(length = 12)
#TLX.perQuestion_std <- matrix(nrow = 6, ncol = 3, byrow = FALSE)
#colnames(TLX.perQuestion_std) <-  c("Question","Pen and Paper", "VR")
#TLX.perQuestion_std_dataframe <- as.data.frame(TLX.perQuestion_std)

TLX_questions <- c("Mental","Physical","Time","Performance","Effort","Frustration")
drawingtypes <- c("PnP","VR")

for(q in TLX_questions){
  TLX.perQuestion_dataframe[match(q,TLX_questions),1] <- q
  TLX.perQuestion_std_dataframe[match(q,TLX_questions),1] <- q
  for (d in drawingtypes){
    TLX.perQuestion_dataframe[match(q,TLX_questions),match(d,drawingtypes)+1] <- mean(questionair[,paste(d,match(q,TLX_questions), sep = "_")])
    TLX.perQuestion_std_dataframe[match(q,TLX_questions),match(d,drawingtypes)+1] <- sd(questionair[,paste(d,match(q,TLX_questions), sep = "_")])
    TLX.perQuestion_std_list[match(q,TLX_questions)*match(d,drawingtypes)] <- sd(questionair[,paste(d,match(q,TLX_questions), sep = "_")])
  }
}

d = melt(TLX.perQuestion_dataframe, id.vars = "Question") #prepare the dataframe for the plot

#TODO:
# y min und y max für confidence intervals and find param for adding these intervals to the plot 95 percent intervalls
# https://r-graph-gallery.com/4-barplot-with-error-bar.html

ggplot(data = d,
       mapping = aes(x = Question, y = value, fill = variable)) + 
    geom_col(position = position_dodge())+
  #TODO: use the 95 percent interval and not the standart deviation
  geom_errorbar( aes(x=Question, ymin=value-TLX.perQuestion_std_list, ymax=value+TLX.perQuestion_std_list), colour="black",position = position_dodge(width = 0.9))
```

## Drawings

```{r drawings}
drawings_2d.import <- read.csv(here("data/Analysis_2D.csv"))# import the csv that contains the analysis data for the 2D drawings
drawings_3d.import <- read.csv(here("data/Analysis_3D.csv")) # import the csv that contains the analysis data for the 3D drawings
duringThesisNotes.import <- read.csv(here("data/DuringThesisNotes.csv"))

# transpose the tables using data.table library
drawings_2d <- transpose(drawings_2d.import) 
drawings_3d <- transpose(drawings_3d.import)
duringThesisNotes <- transpose(duringThesisNotes.import)

# and also shift the col and row names for both lists
rownames(drawings_2d) <- colnames(drawings_2d.import)
colnames(drawings_2d) <- rownames(drawings_2d.import)
rownames(drawings_3d) <- colnames(drawings_3d.import)
colnames(drawings_3d) <- rownames(drawings_3d.import)
rownames(duringThesisNotes) <- colnames(duringThesisNotes.import)
colnames(duringThesisNotes) <- rownames(duringThesisNotes.import)

# after the transpose the correct colnames are in the first row. 
# Use the first row as names and delete the row for both lists
names(drawings_2d)<-drawings_2d[1,]
drawings_2d<-drawings_2d[-1,]
names(drawings_3d)<-drawings_3d[1,]
drawings_3d<-drawings_3d[-1,]
names(duringThesisNotes)<-duringThesisNotes[1,]
duringThesisNotes<-duringThesisNotes[-1,]

#create vectors for saving the sum values of the analysis with the length of the number of participants
#TODO: replace these overall values with f-score
allSum2d <- vector(length=participant_count)
allSum3d <- vector(length=participant_count)

#create more vectors for saving only the visibility sum and the correctness sum
all_Visibility2D <- vector(length=participant_count)
all_Z_Visibility2D <- vector(length=participant_count) #thinking about only analyzing the z visibility
all_Correct2D <- vector(length=participant_count)
all_Visibility3D <- vector(length=participant_count)
all_Correct3D <- vector(length=participant_count)

for(id in 1:participant_count){
  #counter variables for the loop
  sum2d <- 0
  sum2Dvisible <- 0
  sum2Dcorrect <-0
  sum3d <- 0
  sum3Dvisible <- 0
  sum3Dcorrect <-0
  
  # evaluate the object relations
  for(i in 4:129){
    # check how many are correct in 2D
    sum2d <- sum2d + as.numeric(drawings_2d[id,i])
    # check how many are correct in 3D
    sum3d <- sum3d + as.numeric(drawings_3d[id,i])
    # compare only the visibility
    if ( substr(colnames(drawings_2d)[i],1, 1) == "V" ){
      sum2Dvisible <- sum2Dvisible + as.numeric(drawings_2d[id,i])
      sum3Dvisible <- sum3Dvisible + as.numeric(drawings_3d[id,i])
    }
    # compare only the correctness
    #TODO: replace the absolute correctness with relative values depending on the visibility or different
    if ( substr(colnames(drawings_2d)[i],1, 1) == "C" ){
      sum2Dcorrect <- sum2Dcorrect + as.numeric(drawings_2d[id,i])
      sum3Dcorrect <- sum3Dcorrect + as.numeric(drawings_3d[id,i])
    }
  }
  #save the values to the vectors
  allSum2d[id]<-sum2d
  allSum3d[id]<-sum3d
  all_Visibility2D[id]<-sum2Dvisible
  all_Visibility3D[id]<-sum3Dvisible
  all_Correct2D[id]<-sum2Dcorrect
  all_Correct3D[id]<-sum3Dcorrect
}

```

## Linear Mixed Effects Analysis

Now we are going to analyse the data with the use of the linear mixed effects analysis like described in the tutorial by Bodo Winter (https://bodowinter.com/tutorial/bw_LME_tutorial2.pdf)

In a first step we write the sum of visibility relations (visibility.score), the correctness rating (correctness-score) and the results from the f-test, the f-score to a dataframe, adding the ID of the participant, the order of drawing as well as the drawing tool used for the drawing.

```{r Linear Mixed Effects Analysis}
users_data.prep <- matrix(nrow = participant_count*2, ncol = 7, byrow = FALSE)
colnames(users_data.prep) <- c("ID","IPT","DrawingType","order","visibility.score","correctness.score","f.score")
users_data_frame <- as.data.frame(users_data.prep)
#writing the values to the dataframe
for(id in 1:participant_count){
  users_data_frame[id,1] <- as.character(id)
  users_data_frame[id,2] <- ipt_results[id]
  users_data_frame[id,3] <- "2D"
  users_data_frame[id,4] <- if(id %% 2 == 0){"3Dfirst"}else{"2Dfirst"}
  users_data_frame[id,5] <- as.numeric(all_Visibility2D[id])
  users_data_frame[id,6] <- as.numeric(all_Correct2D[id]) #TODO: currently absolute count
  users_data_frame[id,7] <- as.numeric(allSum2d[id]) #TODO: currently not the f-score
  
  users_data_frame[(id+participant_count),1] <- as.character(id)
  users_data_frame[(id+participant_count),2] <- ipt_results[id]
  users_data_frame[(id+participant_count),3] <- "3D"
  users_data_frame[(id+participant_count),4] <- if(id %% 2 == 0){"3Dfirst"}else{"2Dfirst"}
  users_data_frame[(id+participant_count),5] <- all_Visibility3D[id]
  users_data_frame[(id+participant_count),6] <- all_Correct3D[id] #TODO: currently absolute count
  users_data_frame[(id+participant_count),7] <- as.numeric(allSum3d[id]) #TODO: currently not the f-score
}
```

### Visibility of Spatial Relations

Now we have a look at the visibility score as well as setting up the linear mixed effects model to analyse the influence of the drawing tool on the visibility of spatial relations.   

```{r Visibility of Spatial Relations}
#TODO: sharpen the titles and subtitles, labels and styling
ggplot(data    = users_data_frame ,
       aes(x   = IPT,
           y   = visibility.score,
           col = DrawingType,
           group = DrawingType))+ #to add the colors for different classes
  geom_point(linewidth= 1.2,
             alpha    = .8,
             position = "jitter")+ #to add some random noise for plotting purposes
  theme_minimal()+
   geom_smooth(method = lm,
              se     = FALSE,
              size   = .5, 
              alpha  = .8)+ # to add regression line
  labs(title    = "Visibility-Score depending on IPT",
       subtitle = "and in color the different drawing tools")

#setting up the linear mixed effects model for the visible results only
visibility.model = lmer(visibility.score ~ IPT + order + (1|ID) + DrawingType, data=users_data_frame, REML = FALSE)

#TODO: check the interpretation of these
performance::icc(visibility.model)
lme4::isSingular(visibility.model)
performance::check_singularity(visibility.model)

#setting up the null model where the drawing type is omitted
visibility.null = lmer(visibility.score ~ IPT + order + (1|ID), data=users_data_frame, REML = FALSE)
anova(visibility.null,visibility.model)

#residual plot
plot(fitted(visibility.model), resid(visibility.model, type = "pearson"))+
abline(0,0, col="red")
```

In the next section we are trying to get the model for the visibility score without having it singular

```{r, Singular Workaround}
#checking why the model is singular
#the following code comes from: https://stats.stackexchange.com/questions/509892/why-is-this-linear-mixed-model-singular
users_data_frame %>% group_by(ID) %>% summarize(mean = mean(visibility.score))

n.sim <- 100
simvec_rint <- numeric(n.sim)  # vector to hold the random intercepts variances
simvec_fint <- numeric(n.sim)  # vector to hold the fixed intercepts

for (i in 1:n.sim) {
  set.seed(i)
  users_data_frame$visibility.score.1 = users_data_frame$visibility.score + rep(rnorm(6, 0, 1), each = 10)
  m0 <- lmer(visibility.score.1 ~ IPT + order + (1|ID) + DrawingType, data = users_data_frame)

  if (!isSingular(m0)) {
    # If the model is not singular then extract the random and fixed effects
    VarCorr(m0) %>% as.data.frame() %>% pull(vcov) %>% nth(1) -> simvec_rint[i]
    summary(m0) %>% coef() %>% as.vector() %>% nth(1) -> simvec_fint[i]
  } else {
    simvec_rint[i] <- simvec_fint[i] <- NA
  }
}

#TODO or check out baysian lme model https://statmodeling.stat.columbia.edu/2023/06/02/blme-bayesian-linear-mixed-effects-models/
# general check out this: https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#singular-models-random-effect-variances-estimated-as-zero-or-correlations-estimated-as---1 there are different approaches to deal with singular models


#setting up the linear mixed effects model for the visible results only and added randomnes with monte carlo
visibility.modelMonteCarlo = lmer(visibility.score.1 ~ IPT + order + (1|ID) + DrawingType, data=users_data_frame, REML = FALSE)
#setting up the null model
visibility.nullMonteCarlo = lmer(visibility.score.1 ~ IPT + order + (1|ID), data=users_data_frame, REML = FALSE)
#checking the anova for the model with random extended monte carlo model
anova(visibility.nullMonteCarlo,visibility.modelMonteCarlo)

#residual plot
plot(fitted(visibility.modelMonteCarlo), resid(visibility.modelMonteCarlo, type = "pearson"))+
abline(0,0, col="red")
```

### Correctness of Spatial Relations

Now we have a look at the correctness of the spatial relations displayed in the drawings and analyse the influence of the drawing tool on the correctness of the spatial relations.

```{r Correctness of Spatial Relations}
#TODO: use a relative value or something else for the correctness

#have a look at the data with the following plot
ggplot(data    = users_data_frame ,
       aes(x   = IPT,
           y   = correctness.score,
           col = DrawingType,
           group = DrawingType))+ #to add the colors for different classes
  geom_point(size     = 1.2,
             alpha    = .8,
             position = "jitter")+ #to add some random noise for plotting purposes
  theme_minimal()+
   geom_smooth(method = lm,
              se     = FALSE,
              size   = .5, 
              alpha  = .8)+ # to add regression line
  labs(title    = "Correctness-Score depending on IPT",
       subtitle = "and in color the different drawing tools")

#setting up the linear mixed effects model for the correctness score analysis 
correctness.model = lmer(correctness.score ~ IPT + order + (1|ID) + DrawingType, data=users_data_frame, REML = FALSE)
#setting up the null model and check the significants of the model with the anova
correctness.null = lmer(correctness.score ~ IPT + (1|ID) , data=users_data_frame, REML = FALSE)
anova(correctness.null,correctness.model) #checking the significants

#residual plot
plot(fitted(correctness.model), resid(correctness.model, type = "pearson"))+
abline(0,0, col="red")
```
### F-Score

Now that we analysed the visibility and the correctness of the relationships, the f-score will be used to analyse both.

```{r Linear Mixed Effects Analysis}

#TODO: implement the F-Score here and set up the model

#first plot showing the relation of ipt and the general score of the drawing
ggplot(data  = users_data_frame,
       aes(x = IPT,
           y = f.score))+
  geom_point(size = 1.2,
             alpha = .8,
             position = "jitter")+# to add some random noise for plotting purposes
  theme_minimal()+
  labs(title = "IPT vs. General Drawingscore")

#second plot showing is showing the relation of ipt and overall drawing score and additionally in colors the drawing type and the regression line
ggplot(data    = users_data_frame,
       aes(x   = IPT,
           y   = f.score,
           col = DrawingType,
           group = DrawingType))+ #to add the colors for different classes
  geom_point(size     = 1.2,
             alpha    = .8,
             position = "jitter")+ #to add some random noise for plotting purposes
  theme_minimal()+
   geom_smooth(method = lm,
              se     = FALSE,
              size   = .5, 
              alpha  = .8)+ # to add regression line
  labs(title    = "Score depending on IPT",
       subtitle = "and in color the different drawing tools")

#third plot showing is showing the relation of ipt and overall drawing score and additionally in colors the order of drawing tasks and the regression line
ggplot(data    = users_data_frame,
       aes(x   = IPT,
           y   = f.score,
           col = order,
           group = order))+ #to add the colors for different classes
  geom_point(size     = 1.2,
             alpha    = .8,
             position = "jitter")+ #to add some random noise for plotting purposes
  theme_minimal()+
   geom_smooth(method = lm,
              se     = FALSE,
              size   = .5, 
              alpha  = .8)+ # to add regression line
  labs(title    = "Score depending on IPT",
       subtitle = "and in color the order of drawing tools")

#setting up the linear mixed effects model
fscore.model = lmer(f.score ~ IPT + order + 
DrawingType + (1|ID), data=users_data_frame, REML = FALSE)
#setting up the null model that does not contain the Drawing Type
fscore.null = lmer(f.score ~ IPT + order +
(1|ID), data=users_data_frame, REML = FALSE)
#setting up the null model where the ipt is omitted
fscore.nullIPT = lmer(f.score ~ order + DrawingType +
(1|ID), data=users_data_frame, REML = FALSE)
#setting up the null model where the order of drawing is omitted
fscore.nullOrder = lmer(f.score ~ IPT + 
DrawingType + (1|ID), data=users_data_frame, REML = FALSE)

#anova for checking the relevance of the different values
anova(fscore.null,fscore.model) # checking the relevance of the drawing type 
anova(fscore.nullIPT,fscore.model) # checking the relevance of the IPT
anova(fscore.nullOrder,fscore.model) # checking the relevance of the order of the drawing

ggplot(data = users_data_frame, 
       aes(x   = IPT,
           y   = f.score, 
           col = as.factor(DrawingType)))+
  geom_point(size     = 1, 
             alpha    = .7, 
             position = "jitter")+
  geom_smooth(method   = lm,
              se       = T, 
              size     = 1.5, 
              linetype = 1, 
              alpha    = .7)+
  theme_minimal()+
  labs(title    = "Linear Relationship Between Overall Drawing Score and IPT for the 2 Drawingtools")+
  scale_color_manual(name   =" Drawingtool",
                     labels = c("Pen and Paper", "VR"),
                     values = c("darkcyan", "orange"))

#residual plot
plot(fitted(fscore.model), resid(fscore.model, type = "pearson"))+
abline(0,0, col="red")

#TODO: are Statistical tests of normality and equality of variance across groups needed here?
#QQ-plot to check for normality of residuals
#qqnorm(resid(visible_and_correct.model))+
#qqline(resid(visible_and_correct.model), col = "red") # add a perfect fit line
```

```{r Jakub's Code}

#TODO: for additional plots check out: https://www.rensvandeschoot.com/tutorials/lme4/

#load the script created by Jakub Krukar
source(here("options_jk.R"))

#checking which model fits better
#TODO: get deeper into it
model.comparison(visible_and_correct.model, visible_and_correct.null)
model.comparison(visible.model, visible.null)
model.comparison(correct.model,correct.null)

# with flexplot you can plot the distribution of different values. check for example how the IPT results are distributed.
  # flexplot(IPT~1,data=users_data_frame)
  # flexplot(score~1, data=users_data_frame.correct)

#TODO: check this visualization
visualize(visible.model, plot="all")

#plotting the drawingtype effect on the score
#TODO: replot this without the connection line between the 2D and 3D value
plot(effect("DrawingType",visible.model), main="Effect of Drawing Type on Visibility of Spatial Relations", xlab = "Drawing Type", ylab = "Visibility Score", colors = c("darkorange", "darkcyan"))
plot(effect("DrawingType",correct.model))
plot(effect("DrawingType",visible_and_correct.model))
#plotting the IPT effec ton the score
plot(effect("IPT",visible.model))
plot(effect("IPT",correct.model))
plot(effect("IPT",visible_and_correct.model))

#error in this function
lmercheck(visible.model)
lmercheck(correct.model)

#running without error
lmcheck(visible.model)

vif.mer(visible.model)

kappa.mer(visible.model)
kappa.mer(correct.model)

#error
colldiag.mer(visible.model)

#error
maxcorr.mer(visible.model)

#error
view_kable(visible.model)
```

```{r Plots}
visible.plottest = lmer(score ~ IPT + (1|ID) + (1|DrawingType), data=users_data_frame.visible)
estimates(visible.plottest)
visualize(visible.plottest, plot="all",formula = NULL)
#TODO: is there a way to compare these two? 
#TODO: create graphics
```
